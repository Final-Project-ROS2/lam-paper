%%
%% This is file `sample-acmsmall-submission.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,journal,bibtex,acmsmall-submission')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall-submission.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[acmsmall,screen,review]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%%
%% These commands are for a JOURNAL article.
\acmJournal{JACM}
\acmVolume{37}
\acmNumber{4}
\acmArticle{111}
\acmMonth{8}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\usepackage{color}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Architecting Large Action Models for Human-in-the-Loop Intelligent Robots}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Kanisorn Sangchai}
\orcid{0009-0000-5772-1325}
\affiliation{%
  \institution{International School of Engineering, Faculty of Engineering, Chulalongkorn University}
  \city{Bangkok}
  \country{Thailand}}
\email{6538020621@student.chula.ac.th}

\author{Methasit Boonpun}
\orcid{0009-0000-5772-1325}
\affiliation{%
  \institution{International School of Engineering, Faculty of Engineering, Chulalongkorn University}
  \city{Bangkok}
  \country{Thailand}}
\email{methasit.b@chula.ac.th}

\author{Withawin Kraipetchara}
\orcid{0009-0000-5772-1325}
\affiliation{%
  \institution{International School of Engineering, Faculty of Engineering, Chulalongkorn University}
  \city{Bangkok}
  \country{Thailand}}
\email{withawin.k@chula.ac.th}

\author{Krittin Kitjaruwannakul}
\orcid{0009-0000-5772-1325}
\affiliation{%
  \institution{International School of Engineering, Faculty of Engineering, Chulalongkorn University}
  \city{Bangkok}
  \country{Thailand}}
\email{krittin.k@chula.ac.th}

\author{Paulo Garcia}
\orcid{0000-0002-1041-5205}
\affiliation{%
  \institution{International School of Engineering, Faculty of Engineering, Chulalongkorn University}
  \city{Bangkok}
  \country{Thailand}}
\email{paulo.g@chula.ac.th}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Sangchai et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The realization of intelligent robots, operating autonomously and interacting with other intelligent agents, human or artificial, requires the integration of environment perception, reasoning, and action.
Classic Artificial Intelligence techniques for this purpose, focusing on \textit{symbolic} approaches, have long-ago hit the scalability wall on compute and memory costs. Advances in Large Language Models in the past decade (\textit{neural} approaches) have resulted in unprecedented displays of capability, at the cost of control, explainability, and interpretability. 
Large Action Models aim at extending Large Language Models to encompass the full perception, reasoning, and action cycle; requiring substantially more comprehensive training and suffering from the same deficiencies.
Here, we show it is possible to build competent Large Action Models using extant building-blocks, and that their control, interpretability, and explainability can be effected by incorporating symbolic wrappers, and associated verification, on their outputs, achieving \textit{neuro-symbolic} solutions for intelligent robots.
Our experiments on a multi-modal robot show a Large Action Model intelligence can be achieved by the composition of compute-efficient smaller models, where action verification and explainability is driven by the generation and verification of Planning Domain Definition Language code.
These results can support practitioners in the design and development of robotic Large Action Models across novel industries, and shed some light on the the ongoing challenges that must be addressed in the field.


%Two or three sentences to provide a broader perspective, readily
%comprehensible to a scientist in any discipline, may be included in the
%first paragraph if the editor considers that the accessibility of the paper
%is significantly enhanced by their inclusion. 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Do, Not, Use, This, Code, Put, the, Correct, Terms, for,
  Your, Paper}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

\par\textcolor{red}{\textbf{Why Intelligent Robots matter}}
\par\textcolor{red}{\textbf{Why it's difficult: challenges with classic tehniques on scalability and generalization, challenges with LLMs on control and explainability}}
\par\textcolor{red}{\textbf{The promise of LAMs, and how they can be made neuro-symbolic}}

\par Specifically, this paper offers the following contributions:

\begin{itemize}
  \item \textcolor{red}{\textbf{We describe an architecture that allows building competent LAMs without any additional training by leveraging extand smaller models in a hierarchical structure}}
  \item ...
\end{itemize}

\par The remainder of this paper is organized as follows: Section \ref{sec:arch}... \par\textcolor{red}{\textbf{describe structure here when everything else is done}}
\section{Introduction}

Intelligent robots capable of perceiving their surroundings, reasoning about tasks, and executing reliable actions are increasingly essential in engineering, manufacturing, and everyday collaborative settings. Their potential impact lies not only in automating repetitive labor, but in enabling fluid human–robot collaboration where robots understand user intent, adapt to changing environments, and provide transparent, verifiable reasoning. Achieving this, however, requires solving the longstanding integration problem: connecting perception, language, symbolic reasoning, and physical control into a coherent system.

\par Classic \textit{symbolic} AI approaches offer strong guarantees on correctness, interpretability, and verification, but scale poorly with the complexity and variability of real-world environments~\cite{tamp, recent-trends-in-tamp}. Conversely, \textit{neural} methods, particularly Large Language Models (LLMs) and Vision-Language Models (VLMs), excel at perception, abstraction, and generalization, but struggle with controllability, reliability, and the prevention of hallucinations~\cite{eval-application-challenges-llms}. These limitations hinder their deployment in safety-critical robotic systems, where incorrect reasoning can lead to dangerous actions~\cite{plangenllm}.

\par Large Action Models (LAMs) have recently emerged as a transformative paradigm to address these gaps, marking a fundamental shift from passive language processing to active, real-world engagement~\cite{lam}.  Unlike traditional LLMs that are limited to text generation, LAMs are specifically designed to interpret abstract user intentions and generate executable action sequences in dynamic environments~\cite{lam}. However, this transition introduces significant challenges. As noted in recent studies, the ability to manipulate the physical world is a double-edged sword: unlike textual hallucinations, errors in action execution can lead to irreversible physical consequences or safety hazards~\cite{lam}. Furthermore, current LAMs often struggle with scalability and generalizability, typically requiring extensive data collection and retraining when applied to new environments~\cite{lam}. Consequently, a major open question is whether it is possible to construct practical, explainable, and safe LAMs without end-to-end training—using only existing small models and symbolic structures to ensure verification.

\par In this work, we propose that competent LAM intelligence can be achieved by the composition of compute-efficient smaller models, where action verification is driven by symbolic logic. Specifically, this paper offers the following contributions:

\begin{itemize}
  \item \textbf{We describe a neuro-symbolic architecture that constructs a LAM without additional training.} By leveraging extant models (such as SAM and CLIP) in a hierarchical structure, we demonstrate how to decouple perception from reasoning to achieve generalization in unstructured environments.
  \item \textbf{We introduce a mechanism for control and explainability via PDDL generation.} We show that by treating the LLM as a "modeler" that generates Planning Domain Definition Language (PDDL) code, we can enforce symbolic verification on the robot's output, ensuring actions are logically sound before execution~\cite{code-as-symbolic-planner, llm-as-planning-formalizers}.
  \item \textbf{We validate this hypothesis through a prototype engineering assistant.} We provide experimental evidence on a Universal Robots (UR) arm, demonstrating that this composite approach successfully grounds natural language commands into safe, interpretable physical actions.
\end{itemize}

\section{Composing Large Action Model Architectures for Intelligent Robots}\label{sec:arch}

\subsection{LAM componentization}

\par\textcolor{red}{\textbf{Overall description of your architecture, in the most general terms possible. E.g., no mention of SAM specifically: just a mention of any image-segmentation model. This is meant to describe your architecture so anyone else can build the same, using any other components. Describe the hierarchical planner, how it is used to implement a LAM. Images without specific details (e.g., Figure 2 in your report, and Figure 12, as it was in the presentation, without the ROS-specific details)}}

\subsection{Symbolic Wrapping of LAMs}

\par\textcolor{red}{\textbf{Explain how the generation of PDDL (here, without necessarily mentioning PDDL specifically; just any symbolic, formal output) on the output stage allows you to verify, log, and debug any action}}


\section{Case Study: a Human-in-the-Loop Intelligent Robotic Arm}

\par\textcolor{red}{\textbf{Virtually identical to previous section, but now precisely describing your prototype. Here, all components are specified, challenges with specific technologies are described, etc. Here is where you show where, e.g., SAM goes in.}}

\subsection{Perception Module}

\subsection{Automatic Speech Recognition Module}

\subsection{Planning Module}

\section{Experiments and Results}

\par\textcolor{red}{\textbf{Description of all experiments you made, and associated results. Tables and plots. Images of simulation and reality, with links to the associated videos.}}

\subsection{Perception Module}

\subsection{Automatic Speech Recognition Module}

\subsection{Planning Module}


\section{Related Work}
\par\textcolor{red}{\textbf{Description of classic techniques (symbolic only), focusing on PDDL}}
\par\textcolor{red}{\textbf{Description of neural techniques using just LLMs or similar}}
\par\textcolor{red}{\textbf{Other neuro-symbolic / LAM approaches}}

\section{Conclusions}

\par\textcolor{red}{\textbf{Very short paragraph re-stating the contributions of your work; very similar to the "One sentence summarizing the main result" from the abstract}}
\par\textcolor{red}{\textbf{Description of main results, and lessons learned}}
\par\textcolor{red}{\textbf{Future work: what's missing? This isn't what you will do; this is what must be done, by anyone in the world, for this work to be better, given the lessons learned above}}


\par dummy citation so BibTex doesn't complain: \cite{TeXFAQ}

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
The authors would like to thank the Robotics\& AI committee at the International School of Engineering, Chulalongkorn University, for financially supporting this project.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{refs-main}


%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{If needed}

Put stuff here


\end{document}
\endinput
%%
%% End of file `sample-acmsmall-submission.tex'.
