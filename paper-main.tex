%%
%% This is file `sample-acmsmall-submission.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,journal,bibtex,acmsmall-submission')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall-submission.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[acmsmall,screen,review]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%%
%% These commands are for a JOURNAL article.
\acmJournal{JACM}
\acmVolume{37}
\acmNumber{4}
\acmArticle{111}
\acmMonth{8}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\usepackage{color}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Architecting Large Action Models for Human-in-the-Loop Intelligent Robots}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Kanisorn Sangchai}
\orcid{0009-0000-5772-1325}
\affiliation{%
  \institution{International School of Engineering, Faculty of Engineering, Chulalongkorn University}
  \city{Bangkok}
  \country{Thailand}}
\email{6538020621@student.chula.ac.th}

\author{Methasit Boonpun}
\orcid{0009-0000-5772-1325}
\affiliation{%
  \institution{International School of Engineering, Faculty of Engineering, Chulalongkorn University}
  \city{Bangkok}
  \country{Thailand}}
\email{methasit.b@chula.ac.th}

\author{Withawin Kraipetchara}
\orcid{0009-0000-5772-1325}
\affiliation{%
  \institution{International School of Engineering, Faculty of Engineering, Chulalongkorn University}
  \city{Bangkok}
  \country{Thailand}}
\email{withawin.k@chula.ac.th}

\author{Krittin Kitjaruwannakul}
\orcid{0009-0000-5772-1325}
\affiliation{%
  \institution{International School of Engineering, Faculty of Engineering, Chulalongkorn University}
  \city{Bangkok}
  \country{Thailand}}
\email{krittin.k@chula.ac.th}

\author{Paulo Garcia}
\orcid{0000-0002-1041-5205}
\affiliation{%
  \institution{International School of Engineering, Faculty of Engineering, Chulalongkorn University}
  \city{Bangkok}
  \country{Thailand}}
\email{paulo.g@chula.ac.th}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Sangchai et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The realization of intelligent robots, operating autonomously and interacting with other intelligent agents, human or artificial, requires the integration of environment perception, reasoning, and action.
Classic Artificial Intelligence techniques for this purpose, focusing on \textit{symbolic} approaches, have long-ago hit the scalability wall on compute and memory costs. Advances in Large Language Models in the past decade (\textit{neural} approaches) have resulted in unprecedented displays of capability, at the cost of control, explainability, and interpretability. 
Large Action Models aim at extending Large Language Models to encompass the full perception, reasoning, and action cycle; requiring substantially more comprehensive training and suffering from the same deficiencies.
Here, we show it is possible to build competent Large Action Models using extant building-blocks, and that their control, interpretability, and explainability can be effected by incorporating symbolic wrappers, and associated verification, on their outputs, achieving \textit{neuro-symbolic} solutions for intelligent robots.
Our experiments on a multi-modal robot show a Large Action Model intelligence can be achieved by the composition of compute-efficient smaller models, where action verification and explainability is driven by the generation and verification of Planning Domain Definition Language code.
These results can support practitioners in the design and development of robotic Large Action Models across novel industries, and shed some light on the the ongoing challenges that must be addressed in the field.


%Two or three sentences to provide a broader perspective, readily
%comprehensible to a scientist in any discipline, may be included in the
%first paragraph if the editor considers that the accessibility of the paper
%is significantly enhanced by their inclusion. 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Do, Not, Use, This, Code, Put, the, Correct, Terms, for,
  Your, Paper}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Intelligent robots capable of perceiving their surroundings, reasoning about tasks, and executing reliable actions are increasingly essential in engineering, manufacturing, and everyday collaborative settings. Their potential impact lies not only in automating repetitive labor, but in enabling fluid humanâ€“robot collaboration where robots understand user intent, adapt to changing environments, and provide transparent, verifiable reasoning. Achieving this, however, requires solving the longstanding integration problem: connecting perception, language, symbolic reasoning, and physical control into a coherent system.

\par Classic \textit{symbolic} AI approaches offer strong guarantees on correctness, interpretability, and verification, but scale poorly with the complexity and variability of real-world environments~\cite{tamp, recent-trends-in-tamp}. Conversely, \textit{neural} methods, particularly Large Language Models (LLMs) and Vision-Language Models (VLMs), excel at perception, abstraction, and generalization, but struggle with controllability, reliability, and the prevention of hallucinations~\cite{eval-application-challenges-llms}. These limitations hinder their deployment in safety-critical robotic systems, where incorrect reasoning can lead to dangerous actions~\cite{plangenllm}.

\par Large Action Models (LAMs) have recently emerged as a potential solution to this integration gap. Defined as agents trained on large-scale offline datasets via causal sequence modeling, LAMs aim to extend the generalization capabilities of LLMs from text generation to active physical task completion~\cite{lram}. However, relying on massive end-to-end models presents distinct challenges. First, existing LAMs are primarily based on Transformer architectures, which suffer from quadratic inference complexity; this high latency is often prohibitive for real-time robotic control~\cite{lram}. Second, unlike textual hallucinations, errors in action execution ("action hallucinations") can lead to irreversible physical damage and safety hazards~\cite{lam}. Consequently, a major open question is whether it is possible to construct practical LAMs that are computationally efficient, safe, and explainable without the resource-intensive training of massive new sequence models.

\par In this work, we propose that competent LAM intelligence can be achieved by the composition of compute-efficient smaller models, where action verification is driven by symbolic logic. Specifically, this paper offers the following contributions:

\begin{itemize}
  \item \textbf{We describe a modular architecture that achieves LAM capabilities by augmenting LLMs with extant perception models.} By integrating pre-trained vision models (such as SAM and CLIP) to provide the LLM with grounded perception, we demonstrate a method to build competent action models without the need for additional training. This approach not only avoids the costs of end-to-end training but also renders the system's perceptual capabilities more extendable.
  \item \textbf{We introduce a mechanism for control and explainability via PDDL generation.} We show that by treating the LLM as a "modeler" that generates Planning Domain Definition Language (PDDL) code, we can enforce symbolic verification on the robot's output, ensuring actions are logically sound before execution.
  \item \textbf{We validate this hypothesis through a prototype engineering assistant.} We provide experimental evidence on a Universal Robots (UR) arm, demonstrating that this composite approach successfully grounds natural language commands into safe, interpretable physical actions.
\end{itemize}

\par The remainder of this paper is organized as follows: Section \ref{sec:arch}... \par\textcolor{red}{\textbf{describe structure here when everything else is done}}

\section{Composing Large Action Model Architectures for Intelligent Robots}\label{sec:arch}

\subsection{LAM componentization}

We propose a modular, neuro-symbolic architecture that functions as a comprehensive Large Action Model (LAM). Rather than relying on a single, opaque neural network, our system is composed of specialized functional modules that integrate perception, reasoning, and action. As illustrated in Figure~\ref{fig:architecture_overview}, the architecture is defined by a hierarchical planning pipeline driven by multi-modal inputs.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/system_architecture.png}
    \caption{System Architecture Overview. The framework connects high-level cognitive reasoning with deterministic motion control through a stratified planning hierarchy.}
    \label{fig:architecture_overview}
    \Description{System Architecture Overview}{System Architecture Overview. The framework connects high-level cognitive reasoning with deterministic motion control through a stratified planning hierarchy.}
\end{figure}

\textbf{Multi-modal Inputs.}
The system's awareness is driven by two perception modules. The \textit{Perception Module} employs a variety of open-vocabulary foundation models to perform object segmentation, classification, and grasp synthesis. This converts raw pixel data into a useful information without requiring task-specific training. Simultaneously, the \textit{Speech Module} utilizes a neural speech-to-text engine to capture user intent. This interface includes a symbolic override mechanism, where specific safety keywords (e.g., ``Stop'') bypass the reasoning layers to trigger immediate hardware halts.

\textbf{Hierarchical Planning Core.}
To bridge the gap between abstract natural language and precise robot motor control, we implement a three-tier planning hierarchy. This structure ensures that high-level reasoning is grounded in valid physical capabilities.

\begin{itemize}
    \item \textbf{High-Level Reasoning (The Cognitive Agent):} At the top of the hierarchy, the system interprets user commands by leveraging a Large Language Model (LLM), generating a structured sequence of subtasks or actions.
    
    \item \textbf{Medium-Level Orchestration (The Translator):} This layer acts as an intermediary, treating the robot's capabilities as callable ``tools.'' It decomposes the high-level subtasks into specific action sequences if needed. This layer manages the flow of execution and facilitates dynamic feedback between the planner and the perception modules.
    
    \item \textbf{Low-Level Execution (The Controller):} The foundation of the hierarchy is the motion control core. It handles path planning, trajectory generation, and collision avoidance. This layer exposes interfaces for Cartesian and joint-space control, ensuring that the abstract plans received from above are executed with kinematic precision and adherence to safety constraints.
\end{itemize}

This stratified design allows the robot to benefit from the flexibility of modern Generative AI while maintaining the reliability and safety essential for physical interaction.

\subsection{Symbolic Wrapping of LAMs}

\par\textcolor{red}{\textbf{Explain how the generation of PDDL (here, without necessarily mentioning PDDL specifically; just any symbolic, formal output) on the output stage allows you to verify, log, and debug any action}}


\section{Case Study: a Human-in-the-Loop Intelligent Robotic Arm}

\par\textcolor{red}{\textbf{Virtually identical to previous section, but now precisely describing your prototype. Here, all components are specified, challenges with specific technologies are described, etc. Here is where you show where, e.g., SAM goes in.}}

\subsection{Perception Module}

\subsection{Automatic Speech Recognition Module}

\subsection{Planning Module}

\section{Experiments and Results}

\par\textcolor{red}{\textbf{Description of all experiments you made, and associated results. Tables and plots. Images of simulation and reality, with links to the associated videos.}}

\subsection{Perception Module}

\subsection{Automatic Speech Recognition Module}

\subsection{Planning Module}


\section{Related Work}
\par\textcolor{red}{\textbf{Description of classic techniques (symbolic only), focusing on PDDL}}
\par\textcolor{red}{\textbf{Description of neural techniques using just LLMs or similar}}
\par\textcolor{red}{\textbf{Other neuro-symbolic / LAM approaches}}

\section{Conclusions}

\par\textcolor{red}{\textbf{Very short paragraph re-stating the contributions of your work; very similar to the "One sentence summarizing the main result" from the abstract}}
\par\textcolor{red}{\textbf{Description of main results, and lessons learned}}
\par\textcolor{red}{\textbf{Future work: what's missing? This isn't what you will do; this is what must be done, by anyone in the world, for this work to be better, given the lessons learned above}}


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
The authors would like to thank the Robotics\& AI committee at the International School of Engineering, Chulalongkorn University, for financially supporting this project.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{refs-main}


%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{If needed}

Put stuff here


\end{document}
\endinput
%%
%% End of file `sample-acmsmall-submission.tex'.
